# ローカルLLMとGPU・CPU・Windowsライセンス関連まとめ

## 1. LLMとGPU複数枚運用
- 複数GPUは「作業者を増やす」イメージ
- データ並列・モデル並列・パイプライン並列などの方式がある
- GPU間通信や同期で効率は100%にはならない

---

## 2. LLMのローカル実行
- クラウド利用が主流だがローカル実行も可能
- メリット：オフライン利用、カスタマイズ自由、API料金不要
- デメリット：高性能GPU・大容量VRAMが必要

---

## 3. RTX4060 8GB環境での推奨モデル
- **7Bクラス（Q4量子化）**が快適ゾーン
  - Mistral 7B
  - Phi-3 Mini
  - Llama 3/3.1 8B（軽量量子化）
- GUI派はLM Studio、軽量派はOllama推奨

---

## 4. サーバー型とスタンドアロン型の比較
| モード | GPUメモリ使用量 | RAM使用量 | 消費電力 | 特徴 |
|--------|---------------|-----------|---------|------|
| サーバー型（常時稼働） | 約6〜7GB | 約2〜4GB | 70〜120W | API即応答、常時GPU占有 |
| スタンドアロン型（単発実行） | 0GB（未起動時）／6〜7GB（起動時） | 0GB／2〜4GB | Idle 40〜60W／動作中 100〜140W | 必要時のみGPU使用 |

---

## 5. ローカルLLM GPU性能とメモリ量早見表
| モデル名 | パラメータ数 | VRAM目安 | 特徴 |
|----------|-------------|----------|------|
| Phi-3 Mini (3.8B) | 3.8B | 4〜6 GB | 軽量、ノートPCでも可 |
| Mistral 7B | 7B | 8〜12 GB | 高性能、小規模GPUで可 |
| LLaMA 2 13B | 13B | 16〜24 GB | 中規模モデル、詳細な回答 |
| Mixtral 8x7B (MoE) | 47B（活性化時は約13B） | 24〜32 GB | Mixture of Experts構造で高性能 |
| LLaMA 2 70B | 70B | ≥ 80 GB | 超大規模、ほぼクラウド専用 |

---

## 6. RTX4060 8GB環境での推奨モデル
- **7Bクラス（Q4量子化）**が快適ゾーン
  - Mistral 7B
  - Phi-3 Mini
  - Llama 3/3.1 8B（軽量量子化）
- GUI派はLM Studio、軽量派はOllama推奨

---

## 7. WindowsとLinuxでのLLM運用比較
| 項目 | Windows | Linux |
|------|---------|-------|
| 初期セットアップ難易度 | 低 | 中〜高 |
| GPUメモリ効率 | やや低い | 高い |
| 推論速度 | 基準（x1.0） | 約5〜15%高速 |
| 長時間稼働安定性 | 良 | 非常に良 |
| 対応ツール | GUI豊富 | CLI豊富 |
| 普段使いとの両立 | ◎ | △ |

---

## 8. Windowsライセンス移行
- Retail版は別PCに移せる
- 移行手順：
  1. 旧PCでキー解除（`slmgr /upk`、`slmgr /cpky`）
  2. 新PCにインストール
  3. オンライン認証 or 電話認証
- プロダクトキー確認方法（PowerShell）：
  ```powershell
  wmic path softwarelicensingservice get OA3xOriginalProductKey
